# -*- coding: utf-8 -*-
"""Regression_Techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cjv7-Wyltu-2XwdsepXZn6Fk6pz3m58W
"""

from google.colab import files
uploaded = files.upload()
list(uploaded.keys())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Models (5 techniques)
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

RANDOM_STATE = 40
np.random.seed(RANDOM_STATE)

# Helper to print section headers nicely
def h(title):
    print("\n" + "="*len(title))
    print(title)
    print("="*len(title))

granularity = "hour"  #@param ["day", "hour"]
csv_path = "day.csv" if granularity == "day" else "hour.csv"

df = pd.read_csv(csv_path)
h(f"Loaded: {csv_path}  |  shape={df.shape}")
df.head()

h("Basic info")
print(df.info())

h("First 5 rows")
display(df.head())

h("Summary stats (numeric)")
display(df.describe().T)

h("Missing values per column")
print(df.isna().sum())

h("Duplicates")
print("Number of duplicate rows:", df.duplicated().sum())

h("Columns")
print(list(df.columns))

# Parse date for chronological split
df["dteday"] = pd.to_datetime(df["dteday"])

# Drop leakage and IDs
drop_cols = ["instant", "dteday", "casual", "registered"]  # 'cnt' kept as target
drop_cols = [c for c in drop_cols if c in df.columns]

# Identify categorical & numeric columns
categorical_cols = ["season","yr","mnth","holiday","weekday","workingday","weathersit"]
if "hr" in df.columns:  # hourly data has hour column
    categorical_cols.append("hr")

numeric_cols = [c for c in df.columns
                if c not in drop_cols + categorical_cols + ["cnt"]]

h("Columns by type")
print("Categorical:", categorical_cols)
print("Numeric:", numeric_cols)
print("To drop (from features):", drop_cols)

# Build feature/target tables
features = [c for c in df.columns if c not in drop_cols + ["cnt"]]
X = df[features].copy()
y = df["cnt"].astype(float).copy()

df_sorted = df.sort_values("dteday").reset_index(drop=True)
split_index = int(len(df_sorted) * 0.80)
train_idx = df_sorted.index[:split_index]
test_idx  = df_sorted.index[split_index:]

X_train = X.loc[train_idx]
X_test  = X.loc[test_idx]
y_train = y.loc[train_idx]
y_test  = y.loc[test_idx]

h("Train/Test sizes")
print("X_train:", X_train.shape, "X_test:", X_test.shape)
print("y_train:", y_train.shape, "y_test:", y_test.shape)

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ("num", StandardScaler(), numeric_cols)
    ],
    remainder="drop"
)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

def evaluate(name, model, X_tr, y_tr, X_te, y_te):
    model.fit(X_tr, y_tr)
    pred_tr = model.predict(X_tr)
    pred_te = model.predict(X_te)

    metrics = {
        "Model": name,
        "MAE_train": mean_absolute_error(y_tr, pred_tr),
        "RMSE_train": np.sqrt(mean_squared_error(y_tr, pred_tr)),
        "R2_train": r2_score(y_tr, pred_tr),
        "MAE_test": mean_absolute_error(y_te, pred_te),
        "RMSE_test": np.sqrt(mean_squared_error(y_te, pred_te)),
        "R2_test": r2_score(y_te, pred_te)
    }

    preds = pd.DataFrame({
        "y_true": y_te.values,
        "y_pred": pred_te
    })

    # Add date column if available
    if "dteday" in df.columns:
        preds["dteday"] = df.loc[y_te.index, "dteday"].values

    return metrics, preds

def plot_predictions(preds, title):
    plt.figure(figsize=(12,4))

    # Use date column if it exists, else fall back to index
    if "dteday" in preds.columns:
        xaxis = preds["dteday"]
    elif "date" in preds.columns:
        xaxis = preds["date"]
    else:
        xaxis = preds.index

    plt.plot(xaxis, preds["y_true"], label="Actual")
    plt.plot(xaxis, preds["y_pred"], label="Predicted")
    plt.title(title)
    plt.xlabel("Date / Index")
    plt.ylabel("Rentals (cnt)")
    plt.legend()
    plt.tight_layout()
    plt.show()

def plot_residuals(preds, title):
    res = preds["y_true"] - preds["y_pred"]

    plt.figure(figsize=(6,4))
    plt.hist(res, bins=30)
    plt.title(f"{title} â€” Residuals Histogram")
    plt.xlabel("Residual")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(6,4))
    plt.scatter(preds["y_pred"], res, s=10)
    plt.axhline(0, color='r', linestyle='--')
    plt.title(f"{title} â€” Residuals vs Predicted")
    plt.xlabel("Predicted")
    plt.ylabel("Residual")
    plt.tight_layout()
    plt.show()

tscv = TimeSeriesSplit(n_splits=5)

# Model 1 â€” Linear Regression
linreg = Pipeline([
    ("prep", preprocessor),
    ("model", LinearRegression())
])

m1_metrics, m1_preds = evaluate("LinearRegression", linreg, X_train, y_train, X_test, y_test)
pd.DataFrame([m1_metrics])

#Plots: Linear Regression

plot_predictions(m1_preds, "Linear Regression â€” Test Predictions")
plot_residuals(m1_preds, "Linear Regression")

# Model 2 â€” Ridge (with GridSearchCV)

ridge = Pipeline([
    ("prep", preprocessor),
    ("model", Ridge(random_state=RANDOM_STATE))
])

param_grid_ridge = {
    "model__alpha": [0.1, 1.0, 3.0, 10.0, 30.0, 100.0]
}

gs_ridge = GridSearchCV(
    ridge,
    param_grid_ridge,
    scoring="neg_root_mean_squared_error",
    cv=tscv,
    n_jobs=-1
)
m2_metrics, m2_preds = evaluate("Ridge", gs_ridge, X_train, y_train, X_test, y_test)
print("Best params:", gs_ridge.best_params_)
pd.DataFrame([m2_metrics])

# Plots: Ridge

plot_predictions(m2_preds, "Ridge â€” Test Predictions")
plot_residuals(m2_preds, "Ridge")

# Model 3 â€” Lasso (with GridSearchCV)

lasso = Pipeline([
    ("prep", preprocessor),
    ("model", Lasso(random_state=RANDOM_STATE, max_iter=10000))
])

param_grid_lasso = {
    "model__alpha": [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]
}

gs_lasso = GridSearchCV(
    lasso,
    param_grid_lasso,
    scoring="neg_root_mean_squared_error",
    cv=tscv,
    n_jobs=-1
)
m3_metrics, m3_preds = evaluate("Lasso", gs_lasso, X_train, y_train, X_test, y_test)
print("Best params:", gs_lasso.best_params_)
pd.DataFrame([m3_metrics])

# Plots: Lasso
plot_predictions(m3_preds, "Lasso â€” Test Predictions")
plot_residuals(m3_preds, "Lasso")

# Model 4 â€” Random Forest (with GridSearchCV)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Build pipeline
rf = Pipeline([
    ("prep", preprocessor),
    ("model", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))
])

# ðŸ”¹ Reduced grid so training is faster in Colab
param_grid_rf = {
    "model__n_estimators": [100, 200],     # fewer trees
    "model__max_depth": [None, 10],        # only 2 options
    "model__min_samples_leaf": [1, 2]      # keep small
}

# GridSearch with 3 folds instead of 5 (faster)
gs_rf = GridSearchCV(
    rf,
    param_grid_rf,
    scoring="neg_root_mean_squared_error",
    cv=3,
    n_jobs=-1,
    verbose=1
)

# Evaluate
m4_metrics, m4_preds = evaluate("RandomForest", gs_rf, X_train, y_train, X_test, y_test)

print("Best params:", gs_rf.best_params_)
pd.DataFrame([m4_metrics])

# Plots: Random Forest
plot_predictions(m4_preds, "Random Forest â€” Test Predictions")
plot_residuals(m4_preds, "Random Forest")

# Model 5 â€” Gradient Boosting (with GridSearchCV)
gbr = Pipeline([
    ("prep", preprocessor),
    ("model", GradientBoostingRegressor(random_state=RANDOM_STATE))
])

param_grid_gbr = {
    "model__n_estimators": [200, 400],
    "model__learning_rate": [0.03, 0.05, 0.1],
    "model__max_depth": [2, 3]
}

gs_gbr = GridSearchCV(
    gbr,
    param_grid_gbr,
    scoring="neg_root_mean_squared_error",
    cv=tscv,
    n_jobs=-1
)

m5_metrics, m5_preds = evaluate("GradientBoosting", gs_gbr, X_train, y_train, X_test, y_test)
print("Best params:", gs_gbr.best_params_)
pd.DataFrame([m5_metrics])

#Plots: Gradient Boosting
plot_predictions(m5_preds, "Gradient Boosting â€” Test Predictions")
plot_residuals(m5_preds, "Gradient Boosting")

results = pd.DataFrame([m1_metrics, m2_metrics, m3_metrics, m5_metrics])
display(results.sort_values("RMSE_test"))

order = results.sort_values("RMSE_test")
plt.figure(figsize=(7,4))
plt.bar(order["Model"], order["RMSE_test"])
plt.title("Test RMSE by Model")
plt.ylabel("RMSE")
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

def show_top_features_fitted(fitted_pipeline, top_k=15, title=""):
    # If input is GridSearchCV, extract the pipeline
    if hasattr(fitted_pipeline, "best_estimator_"):
        pipeline = fitted_pipeline.best_estimator_
    else:
        pipeline = fitted_pipeline

    prep = pipeline.named_steps["prep"]
    model = pipeline.named_steps["model"]
    names = get_feature_names(prep, X_train)

    if hasattr(model, "coef_"):
        coefs = model.coef_
        imp = pd.Series(coefs, index=names).sort_values(key=np.abs, ascending=False)
        display(imp.head(top_k).to_frame("coefficient"))
        imp_top = imp.head(top_k)
        plt.figure(figsize=(8,5))
        plt.barh(imp_top.index[::-1], imp_top.values[::-1])
        plt.title(title + " â€” Top coefficients (std. units)")
        plt.tight_layout()
        plt.show()

    elif hasattr(model, "feature_importances_"):
        fi = pd.Series(model.feature_importances_, index=names).sort_values(ascending=False)
        display(fi.head(top_k).to_frame("importance"))
        fi_top = fi.head(top_k)
        plt.figure(figsize=(8,5))
        plt.barh(fi_top.index[::-1], fi_top.values[::-1])
        plt.title(title + " â€” Top feature importances")
        plt.tight_layout()
        plt.show()
    else:
        print("This model doesn't expose coefficients or importances.")
show_top_features_fitted(linreg, top_k=15, title="Linear Regression")
show_top_features_fitted(gs_ridge, top_k=15, title="Ridge")
show_top_features_fitted(gs_lasso, top_k=15, title="Lasso")
# show_top_features_fitted(gs_rf, top_k=15, title="Random Forest")
show_top_features_fitted(gs_gbr, top_k=15, title="Gradient Boosting")

# Save predictions of best model by RMSE
best_name = results.sort_values("RMSE_test").iloc[0]["Model"]
print("Best by RMSE:", best_name)

name_to_preds = {
    "LinearRegression": m1_preds,
    "Ridge": m2_preds,
    "Lasso": m3_preds,
    # "RandomForest": m4_preds,
    "GradientBoosting": m5_preds
}
best_preds = name_to_preds[best_name].copy()
best_preds.to_csv(f"{granularity}_best_model_predictions.csv", index=False)
best_preds.head()