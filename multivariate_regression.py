# -*- coding: utf-8 -*-
"""LAb-5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18xfbsSRL9oEtalRo1pHto7KJMnwQqa4f
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

columnames=["size","bedrooms","price"]
dataset=pd.read_csv("https://raw.githubusercontent.com/nishithkotak/machine-learning/refs/heads/master/ex1data2.txt",names=columnames)

dataset

"""dataset.describe()"""

area=dataset.iloc[0:dataset.shape[0],0:1]
rooms=dataset.iloc[0:dataset.shape[0],1:2]
price=dataset.iloc[0:dataset.shape[0],2:3]

dataset.shape

#function of normalization
def featureNormalize(x):
    mean=np.mean(x,axis=0)
    std=np.std(x,axis=0)
    X_normalized=(x-mean)/std
    return X_normalized,mean,std

data_norm=dataset.values
m=data_norm.shape[0]
#take the feature vectors
x2=data_norm[:,0:2].reshape(m,2)
x2_norm,mean,std=featureNormalize(x2)
y2=data_norm[:,2:3].reshape(m,1) #price column

x2_norm

theta_array=np.zeros((3,1))

def Hypothesis(theta_array, x1, x2):
    return theta_array[0] + theta_array[1]*x1 + theta_array[2]*x2

def Cost_Function(theta_array, x1, x2, y, m):
    total_cost = 0
    for i in range(m):
        prediction = Hypothesis(theta_array, x1[i], x2[i])
        total_cost += (prediction - y[i]) ** 2
    return total_cost / (2 * m)

def Gradient_Descent(theta_array, x1, x2, y, m, alpha):
    sum0 = 0
    sum1 = 0
    sum2 = 0
    for i in range(m):
        prediction = Hypothesis(theta_array, x1[i], x2[i])
        error = prediction - y[i]
        sum0 += error
        sum1 += error * x1[i]
        sum2 += error * x2[i]
    new_theta0 = theta_array[0] - (alpha / m) * sum0
    new_theta1 = theta_array[1] - (alpha / m) * sum1
    new_theta2 = theta_array[2] - (alpha / m) * sum2
    return [new_theta0, new_theta1, new_theta2]

def Training(x, y, alpha, iters):
    m = x.shape[0]
    theta_array = [0, 0, 0]  # For theta0, theta1, theta2
    cost_values = []

    x1 = x[:, 0]
    x2 = x[:, 1]

    for i in range(iters):
        theta_array = Gradient_Descent(theta_array, x1, x2, y, m, alpha)
        cost = Cost_Function(theta_array, x1, x2, y, m)
        cost_values.append(cost)

    return theta_array, cost_values

Training_data = dataset.dropna()

Training_data.shape

x_value = Training_data[["size", "bedrooms"]].values
y_value = Training_data[["price"]].values

x_value, mean, std = featureNormalize(x_value)

type(x_value), type(y_value)  # Both should be <class 'numpy.ndarray'>

alpha = 0.1
iters = 50
theta_array, cost_values = Training(x_value, y_value, alpha, iters)

cost_values = [val[0] if isinstance(val, (list, np.ndarray)) else val for val in cost_values]

cost_values = np.array(cost_values).flatten()

x_axis = np.arange(0, len(cost_values), step=1)
plt.plot(x_axis, cost_values)
plt.xlabel("Iterations")
plt.ylabel("Cost Values")
plt.title("Loss Graph")
plt.show()
